"""
===========================
pipeline_humann3.py
===========================

Overview
========

This pipeline uses humann3 to take raw fastq files and estimate 
taxonomic abundunce, then functionally profile the metagenome.
files :file:``pipeline.yml` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use cgat pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_humann.py config

Input files
-----------

Input files should be fastq.gz
Paired-end reads should be concatenated into one fastq.gz file. 
Can use concat_fastq.py for this -- paired end fastq files are 
in the format .fastq.1.gz and .fastq.2.gz.

Requirements
------------


Dependencies
============
Bowtie2
DIAMOND
R
Pandoc

module load Bowtie2/2.4.1-GCC-9.3.0
module load DIAMOND/0.9.36-GCC-9.3.0
module load R/4.0.0-foss-2020a
module load R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0
module load Pandoc/2.13

Pipeline output
===============


Glossary
========

.. glossary::


Code
====

"""
import sys
import os
import glob
import re
import shutil
from ruffus import *
from cgatcore import pipeline as P

import ocmsshotgun.modules.Utility as utility
import ocmsshotgun.modules.Humann3 as H

# load options from the config file
PARAMS = P.get_parameters(
    ["pipeline.yml"])

# check all files to be processed
FASTQ1s = utility.check_input(PARAMS['location_input'])

if PARAMS['location_transcriptome']:
    FASTQ2s = utility.check_input(PARAMS['location_transcriptome'])
else:
    FASTQ2s = None

###############################################################################
# Run humann3 on concatenated fastq.gz
###############################################################################
@follows(mkdir("input_merged.dir"))
@transform(FASTQ1s,
           regex(".+/(.+).fastq.1.gz"),
           r"input_merged.dir/\1.fastq.gz")
def poolInputFastqs(infile, outfile):
    '''Humann relies on pooling input files'''

    infiles = utility.matchReference(infile, outfile, **PARAMS)
    fastqs = [i for i in [infiles.fastq1, infiles.fastq2, infiles.fastq3] if i]

    if len(fastqs) == 1:
        utility.symlink(infile, outfile)
    else:
        fastqs = ' '.join(fastqs)
        statement = "cat %(fastqs)s > %(outfile)s"
        P.run(statement)


###############################################################################
# Run humann3 on concatenated fastq.gz
# produces a humann3.dir with a folder for each sample, which contains 
# pathcoverage, pathabundance and genefamilies files.
###############################################################################
@follows(mkdir("humann3.dir"))
@subdivide(poolInputFastqs,
           regex(".+/(.+).fastq.gz"),
           r"humann3.dir/\1/\1_*.tsv.gz")
def runHumann3(infile, outfiles):
    '''functional profile with humann3'''
    
    outfile = P.snip(infile, '.fastq.gz', strip_path=True)
    outfile = os.path.join('humann3.dir',
                            outfile,
                            outfile + '_pathcoverage.tsv.gz')
    
    statement = H.humann3(infile, outfile, **PARAMS).run()    
        
    P.run(statement,
          job_memory = PARAMS["humann3_job_memory"],
          job_threads = PARAMS["humann3_job_threads"],
          job_options = PARAMS.get('humann3_job_options',''))

    
###############################################################################
# Run humann3 on metatranscriptome data
###############################################################################
@active_if(PARAMS['location_transcriptome'])    
@follows(mkdir("input_mtx_merged.dir"))
@transform(FASTQ2s,
           regex(".+/(.+).fastq.1.gz"),
           r"input_mtx_merged.dir/\1.fastq.gz")
def poolTranscriptomeFastqs(infile, outfile):
    '''Humann relies on pooling input files'''

    infiles = utility.matchReference(infile, outfile, **PARAMS)
    fastqs = [i for i in [infiles.fastq1, infiles.fastq2, infiles.fastq3] if i]

    if len(fastqs) == 1:
        utility.symlink(infile, outfile)
    else:
        fastqs = ' '.join(fastqs)
        statement = "cat %(fastqs)s > %(outfile)s"
        P.run(statement)


@follows(runHumann3)
@follows(mkdir("humann3_mtx.dir"))
@subdivide(poolTranscriptomeFastqs,
           regex(".+/(.+).fastq.gz"),
           add_inputs(r"humann3.dir/\1/\1_metaphlan_bugs_list.tsv.gz"),
           r"humann3_mtx.dir/\1/\1_*.tsv.gz")
def runHumann3_metatranscriptome(infiles, outfiles):
    '''Optionally run humann3 on matched metatranscriptome data, making 
    use of the metaphlan_bugs_list file output from the metagenome analysis'''

    infile, tax_profile = infiles
    assert os.path.exists(tax_profile), 'No humann3 metagenome output found'
    
    outfile = P.snip(infile, '.fastq.gz', strip_path=True)
    outfile = os.path.join('humann3_mtx.dir',
                            outfile,
                            outfile + '_pathcoverage.tsv.gz')

    # Not sure if humann can take gzipped input
    tmpf = P.get_temp_filename('.')
    statement = "zcat %(tax_profile)s > %(tmpf)s && " % locals()
    statement = statement + H.humann3(infile, outfile,
                                      taxonomic_profile=tmpf, **PARAMS).run()
    # print(statement + '\n')
    
    P.run(statement,
          job_memory = PARAMS["humann3_job_memory"],
          job_threads = PARAMS["humann3_job_threads"],
          job_options = PARAMS.get('humann3_job_options',''))
    os.unlink(tmpf)


###############################################################################
# merge Humann3 output files
###############################################################################
@collate([runHumann3, runHumann3_metatranscriptome],
         regex("(humann3.dir|humann3_mtx.dir)/.+/.+_(pathcoverage|pathabundance|genefamilies).tsv.gz"),
         r"\1/tables/merged_\2.tsv.gz")
def mergeHumannOutput(infiles, outfile):
    '''Merge respective output files from humann. Note this uses humann
    scripts which don't account for the metaphlan bugs list'''
    
    # Fetch the output filetype
    suffix = re.search(".+/.+_(pathcoverage|pathabundance|genefamilies).tsv.gz",
                       infiles[0]).group(1)
    assert suffix in ('pathcoverage', 'pathabundance', 'genefamilies')

    # Fetch the output directory
    outdir = os.path.dirname(outfile)
    indir = os.path.dirname(outdir)
    if os.path.exists(outdir):
        shutil.rmtree(outdir)
    os.mkdir(outdir)

    outf = P.snip(outfile, '.gz')
    
    statement = ("humann_join_tables"
                 "  -i %(indir)s"
                 "  -s"
                 "  --file_name %(suffix)s"
                 "  -o %(outf)s &&"
                 " gzip %(outf)s")
    P.run(statement)


@transform(mergeHumannOutput,
           suffix('genefamilies.tsv.gz'),
           'KOs.tsv.gz')
def mapUniref2KOs(infile, outfile):
    '''Use humann regroup_table script to fetch KO for Uniref mapping'''

    statement = ("zcat %(infile)s |"
                 " humann_regroup_table"
                 "  --custom %(humann3_uniref_to_ko)s"
                 " 2> %(outfile)s.log |"
                 " gzip > %(outfile)s")
    P.run(statement)


@transform([mergeHumannOutput, mapUniref2KOs],
           suffix('.tsv.gz'),
           '_cpm.tsv')
def renormalizeHumannOutput(infile, outfile):
    '''Renormalize to CPM'''

    statement = ("zcat %(infile)s |"
                 " humann_renorm_table"
                 "  --units cpm"
                 " 2> %(outfile)s.log |"
                 " gzip > %(outfile)s")
    P.run(statement)

###############################################################################
# Handle metaphlan output
###############################################################################
@follows(mkdir('metaphlan_output.dir'))
@merge(runHumann3,
         regex("humann3.dir/.+/.+_metaphlan_bugs_list.tsv.gz"),
         r"metaphlan_output.dir/merged_metaphlan_bugs_list.tsv.gz")
def mergeMetaphlanOutput(infiles, outfile):
    '''Merge respective output files from humann's internal metaphlan run.
    Note this uses metaphlan custom scripts.'''

    infiles = ' '.join([os.path.abspath(x) for x in infiles])

    statement = "merge_metaphlan_tables.py -o %(outfile)s %(infiles)s"
    P.run(statement)


@split(mergeMetaphlanOutput, "metaphlan_output.dir/metaphlan_*.tsv")
def splitMetaphlanByTaxonomy(infile, outfiles):
    '''split merged metaphlan file by taxonomic levels'''
    
    statement = '''ocms_shotgun split_metaphlan -i %(infile)s -o humann3.dir'''
    P.run(statement)

# ###############################################################################
# @follows("mergePathAbundance",
#          "mergePathCoverage", 
#          "mergeGeneFamilies",
#          "splitMetaphlan",
#          mkdir("report.dir"))
# def build_report():
#     '''
#     render the rmarkdown report file
#     '''
#     reportdir = os.path.dirname(os.path.abspath(__file__))
#     reportdir = os.path.join(reportdir, "pipeline_docs", "Rmd", "pipeline_humann3")
#     reportfile = os.path.join(reportdir, "pipeline_humann3_report.Rmd")

#     # copy report template to report.dir and render report
#     statement = '''cd report.dir;
#                    cp %(reportfile)s .;
#                    R -e "rmarkdown::render('pipeline_humann3_report.Rmd', output_file='pipeline_humann3_report.html')";
#                    cd ../
#                 '''
#     P.run(statement)

###############################################################################
# Generic pipeline tasks
@follows(renormalizeHumannOutput, splitMetaphlanByTaxonomy)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
