"""
===========================
pipeline_mimicry_search.py
===========================

:Author: Holly Roach and Marcin Pekalski
:Tags: Python

Overview
========
This pipeline takes contig.fasta files as input and uses prokka to predict
open reading frames and generate protein assemblies. The .faa file outputs from
prokka are used to create a blast database, which is used to search for sequence
homology for a specified protein sequence.

Prokka documentation: https://github.com/tseemann/prokka


Usage
=====
Script takes in all contigs.fasta files located in the input.dir, and runs prokka
on these files to return a protein assembly. The .faa files are used as input for
blast to create a blast database, which can then be searched for sequence homology.


Example::
    ocms_shotgun pipeline_mimicry_search make full


Configuration
-------------
ocms_shotgun pipeline_mimicry_search config

Input files
-----------
Input files should be contigs.fasta files located in input.dir

Requirements
------------
prokka/1.14.5-gompi-2022b
BLAST+/2.14.0-gompi-2022b

Pipeline output
===============
prokka_output.dir contains a merged.faa containing protein CDS sequences for all samples, as well as a folder for each
sample which contains the individual outputs for each sample generated by prokka:
    sample_id.faa - Protein FASTA file of the translated CDS sequences. 
    sample_id.ffn - Nucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA)
    sample_id.fsa - Nucleotide FASTA file of the input contig sequences, used by "tbl2asn" to create the .sqn file.
                    It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines.
    sample_id.gff - This is the master annotation in GFF3 format, containing both sequences and annotations. It can be 
                    viewed directly in Artemis or IGV.
    sample_id.sqn - An ASN1 format "Sequin" file for submission to Genbank. It needs to be edited to set the correct 
                    taxonomy, authors, related publication etc.
    sample_id.tsv - Tab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product
    sample_id.err - Unacceptable annotations - the NCBI discrepancy report.
    sample_id.fna - Nucleotide FASTA file of the input contig sequences.
    sample_id.gbk - This is a standard Genbank file derived from the master .gff. If the input to prokka was a multi-FASTA,
                    then this will be a multi-Genbank, with one record for each sequence.
    sample_id.log - Contains all the output that Prokka produced during its run. This is a record of what settings you used, 
                    even if the --quiet option was enabled.
    sample_id.tbl - Feature Table file, used by "tbl2asn" to create the .sqn file.
    sample_id.txt - Statistics relating to the annotated features found.




Glossary
========

..glossary::


Code
====

"""

import sys
import os
import re
import glob
from pathlib import Path
from ruffus import *
from cgatcore import pipeline as P 

# get all fasta contig files within directory to process
FASTAFILES = ("input.dir/*contigs.fasta")
FASTAFILES_REGEX = regex(r"input\.dir\/(\S+?)_.+\.fasta")

PARAMS = P.get_parameters(['pipeline.yml'])


###############################################################################
# Create protein assemblies using prokka
# produces a prokka_output.dir which contains
# predicted protein assembilies
###############################################################################

@follows(mkdir("prokka_output.dir"))
@transform(FASTAFILES, 
         FASTAFILES_REGEX,
         r"prokka_output.dir/\1")

def run_prokka(infile, outfile):
    """Use prokka to predict ORFs and generate protein assemblies"""

    # capture sample id from output dir name
    sample_id = re.sub("prokka_output.dir/", "", outfile)   

    # create statment for running prokka
    # prokka --cpus 0 --prefix {wildcards.sample} --locustag {wildcards.sample} --outdir {output.dir} --force {input}
    statement = f"""prokka --cpus 0 --prefix {sample_id} --locustag {sample_id} --outdir {outfile} --force {infile}"""

    # create script for slurm job submission
    P.run(statement,
          job_threads = PARAMS['job_threads'],
          job_memory = PARAMS['job_memory'])
    

###############################################################################
# Merge all protein assemblies
# protein assemblies for each sample are stored as:
#   prokka_output.dir/FD30232468_masked/FD30232468_masked.faa
###############################################################################
# @follows(mkdir("prokka_output.dir/merged_faa") )
# @collate(run_prokka,
#         regex(r"prokka_output\.dir\/[^\/]+\/(.+\.faa)"),
#         r"prokka_output.dir/merged_faa/merged.faa")
# @collate(run_prokka,
#          regex("prokka_output.dir/.+/.+.faa"),
#          r"prokka_output.dir/merged_faa/merged.faa")

@follows(run_prokka)
@collate("prokka_output.dir/*/*.faa",
         regex("prokka_output.dir/.+/.+.faa"),
         r"prokka_output.dir/merged.faa")

def merge_faa(infile, outfile):
    """Concatenate each faa file into one file"""

    print(f"Infile: {infile}")
    print(f"outfile: {outfile}")

    # create command line statment to concatenate files
    f"cat {' '.join(infiles)} > {outfile}"

    print(statement)
    
    # P.run(statement)



@follows(merge_faa)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(main(sys.argv))
