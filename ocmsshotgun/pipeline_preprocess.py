"""
================================
Preprocess shotgun sequence data
================================


Overview
========

This pipeline is based on the original HMP protocol for
preprocessing mWGS reads: 

1) Remove identical duplicates on the assumption that they are PCR dups. 
2) Trim adapters 
3) Optionally remove rRNA reads (for metatranscriptome)
4) Remove host reads 
5) Either softmask or remove low complexity

Configuration
=============
The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:
    
    python <srcdir>/pipeline_preprocessing.py config

Dependencies
============

cdhit-dup
trimmomatic
sortmeRNA
bmtagger
bbduk

source /well/kir/config/modules.sh
module load CD-HIT/4.8.1-GCC-9.3.0
module load CD-HIT-auxtools/4.8.1-GCC-9.3.0
module load bmtagger/3.101-gompi-2020a
module load Trimmomatic/0.39-Java-11
module load BBMap/38.90-GCC-9.3.0
module load SortMeRNA/4.3.4

Code
====

"""
from ruffus import *
from cgatcore import pipeline as P
from cgatcore import iotools as IOTools
from cgatcore import experiment as E

import cgat.Fastq as Fastq

import os,sys,re
import sqlite3
import itertools
import distutils
import pandas as pd


import ocmsshotgun.modules.PreProcess as pp

# set up params
PARAMS = P.get_parameters(["pipeline.yml"])

# check that input files correspond
FASTQ1S = pp.utility.check_input()

###############################################################################
# Deduplicate
###############################################################################
@follows(mkdir('reads_deduped.dir'))
@transform(FASTQ1S,
           regex(r'.+/(.+).fastq.1.gz'),
           r"reads_deduped.dir/\1_deduped.fastq.1.gz")
def removeDuplicates(fastq1, outfile):
    '''Filter exact duplicates, if specified in config file'''
    pp.cdhit(fastq1, outfile, **PARAMS).run()

###############################################################################
# Remove Adapters
###############################################################################
@follows(mkdir('reads_adaptersRemoved.dir'))
@transform(removeDuplicates,
           regex(r'.+/(.+)_deduped.fastq.1.gz'),
           r'reads_adaptersRemoved.dir/\1_deadapt.fastq.1.gz')
def removeAdapters(fastq1, outfile1):
    '''Remove adapters using Trimmomatic'''

    pp.trimmomatic(fastq1, outfile1, **PARAMS).run()


###############################################################################
# Remove Contamination
###############################################################################
@follows(mkdir('reads_rrnaRemoved.dir'))
@transform(removeAdapters,
           regex(r'.+/(.+)_deadapt.fastq.1.gz'),
           r'reads_rrnaRemoved.dir/\1_rRNAremoved.fastq.1.gz')
def removeRibosomalRNA(fastq1, outfile):
    '''Remove ribosomal RNA using sortMeRNA'''
    

    if PARAMS['data_type'] == 'metatranscriptome':
        tool = pp.runSortMeRNA(fastq1, outfile, **PARAMS)
        tool.run()
        tool.postProcess()
        #pp.removeContaminants(fastq1, 
        #                      outfile, method='sortmerna',
        #                      **PARAMS)
    else:
        assert PARAMS['data_type'] == 'metagenome', \
            'Unrecognised data type: {}'.format(PARAMS['data_type'])
        
        inf1 = fastq1
        inf2 = P.snip(inf1, '.fastq.1.gz') + '.fastq.2.gz'
        inf3 = P.snip(inf1, '.fastq.1.gz') + '.fastq.3.gz'

        outf1 = outfile
        outf2 = P.snip(outf1, '.fastq.1.gz') + '.fastq.2.gz'
        outf3 = P.snip(outf1, '.fastq.1.gz') + '.fastq.3.gz'
        
        pp.utility.symlnk(inf1, outf1)
        if os.path.exists(inf2):
            pp.utility.symlnk(inf2, outf2)
        if os.path.exists(inf3):
            pp.utility.symlnk(inf3, outf3)


@follows(mkdir('reads_rrnaClassified.dir'))
@transform(removeAdapters,
           regex(r'.+/(.+)_deadapt.fastq.1.gz'),
           r'reads_rrnaClassified.dir/\1_otu_map.txt')
def classifyRibosomalRNA(fastq1, outfile):

    assert PARAMS['data_type'] == 'metatranscriptome', \
        "Can't run rRNA classification on mWGS data..."

    tool = pp.createSortMeRNAOTUs(fastq1, 
                                  outfile, 
                                  **PARAMS)
    tool.run()
    

@transform(classifyRibosomalRNA, suffix('_map.txt'), 's.tsv.gz')
def summarizeRibosomalRNAClassification(infile, outfile):
    '''Count the number of reads mapping to each taxon'''
    
    sample_id = P.snip(infile, '_otu_map.txt', strip_path=True)
    
    with IOTools.open_file(outfile, 'w') as outf:
        outf.write('taxonomy\t%s\n' % sample_id)
        for otu in IOTools.open_file(infile):
            taxonomy = otu.split()[0]
            reads = otu.split()[1:]
            outf.write(taxonomy + '\t' + str(len(reads)) + '\n')


@merge(summarizeRibosomalRNAClassification,
       'reads_rrnaClassified.dir/metatranscriptome_otus.tsv.gz')
def combineRNAClassification(infiles, outfile):
    '''Combine output of sortmerna read classification'''

    infiles = ' '.join(infiles)

    statement = ("cgat combine_tables"
                 "  --log=%(outfile)s.log"
                 "  %(infiles)s |"
                 " gzip > %(outfile)s")
    P.run(statement, to_cluster=False)

@follows(mkdir('reads_hostRemoved.dir'))
@transform(removeRibosomalRNA,
           regex('.+/(.+)_rRNAremoved.fastq.1.gz'),
           r'reads_hostRemoved.dir/\1_dehost.fastq.1.gz')
def removeHost(fastq1, outfile):
    '''Remove host contamination using bmtagger'''

    pp.bmtagger(fastq1, outfile, **PARAMS).run()


###############################################################################
# Mask or Remove Low-complexity sequence
###############################################################################
@follows(mkdir('reads_dusted.dir'))
@transform(removeHost,
           regex(r'.+/(.+)_dehost.fastq.1.gz'),
           r'reads_dusted.dir/\1_masked.fastq.1.gz')
def maskLowComplexity(fastq1, outfile):
    '''Either softmask low complexity regions, or remove reads with a large
    proportion of low complexity. 

    Uses BBTools scripts bbduk.sh (removal), or bbmask.sh. 

    Entropy is calculated as shannon entropy for of kmers with a specified size
    within a sliding window. Ranges from 0: mask nothing, 0.0001: mask
    homopolymers, 1: mask everything.
    '''
    pp.bbtools(fastq1, outfile, **PARAMS).run()

###############################################################################
# Summary Metrics
###############################################################################
# @transform(removeAdapters, '.fastq.1.gz', '_histogram.png')
# def plotDeadaptLengthDistribution(infile, outfile):
#     '''Create a histogram of length distributions'''
@follows(mkdir('read_count_summary.dir'))
@transform(FASTQ1S,
           regex(r'.+/(.+).fastq.1.gz'),
           r"read_count_summary.dir/\1_input.nreads")
def countInputReads(infile, outfile):
    
    statement = ("zcat %(infile)s |"
                 " awk '{n+=1;} END {printf(n/4\"\\n\");}'"
                 " > %(outfile)s")

    P.run(statement)


@follows(countInputReads)
@transform([removeDuplicates, removeAdapters, removeRibosomalRNA,
            removeHost, maskLowComplexity],
           regex(r'.+/(.+).fastq.1.gz'),
           r'read_count_summary.dir/\1.nreads')
def countOutputReads(infile, outfile):
    '''Count the number of reads in the output files'''    
    statement = ("zcat %(infile)s |"
                 " awk '{n+=1;} END {printf(n/4\"\\n\");}'"
                 " > %(outfile)s")

    P.run(statement)

@collate([countInputReads, countOutputReads],
         regex(r'(.+)_(input|deduped|deadapt|dehost|rRNAremoved|masked).nreads'),
         r'\1_read_count_summary.tsv')
def collateReadCounts(infiles, outfile):
    '''Collate read counts for each sample'''

    infiles = ' '.join(infiles)
    
    statement = ("cgat combine_tables"
                 " --cat Step"
                 " --regex-filename='.+_(.+)\.nreads'"
                 " --no-titles"
                 " --log=%(outfile)s.log"
                 " %(infiles)s"
                 " > %(outfile)s")
    P.run(statement)
    
@merge(collateReadCounts, 'processing_summary.tsv')
def summarizeReadCounts(infiles, outfile):
    '''Calculate the number of reads lost at each step for each sample'''

    with IOTools.open_file(outfile, 'w') as outf:
        outf.write("sample_id\tinput_reads\toutput_reads\tduplicates\t"
                   "adapter_contamination\trRNA\thost\tlow_complexity\t"
                   "duplicates_percent\tadapters_percent\trrna_percent\t"
                   "host_percent\tlow_complexity_perc\tremaining_percent\n")
        for infile in infiles:
            sample_id = P.snip(os.path.basename(infile),
                               '_read_count_summary.tsv')
            E.info('Processing sample %s' % sample_id)
            
            df = pd.read_table(infile, index_col=0, header=None)
            deadapt = df.loc['deadapt', 1]
            deduped = df.loc['deduped', 1]
            rrna = df.loc['rRNAremoved', 1]
            dehost = df.loc['dehost', 1]
            masked = df.loc['masked', 1]
            input_reads = df.loc['input', 1]
            
            lost_dup = input_reads - deduped
            lost_adapt = deduped - deadapt
            lost_rrna = deadapt - rrna
            lost_host = rrna - dehost
            lost_mask = dehost - masked

            lost_dup_perc = round(lost_dup/float(input_reads) * 100, 2)
            lost_adapt_perc = round(lost_adapt/float(input_reads) * 100, 2)
            lost_rrna_perc = round(lost_rrna/float(input_reads) * 100, 2)
            lost_host_perc = round(lost_host/float(input_reads) * 100, 2)
            lost_mask_perc = round(lost_mask/float(input_reads) * 100, 2)
            output_perc = round(masked/float(input_reads) * 100, 2)

            outf.write('\t'.join(map(str, [sample_id, input_reads, masked, 
                                           lost_dup, lost_adapt, lost_rrna, 
                                           lost_host, lost_mask, lost_dup_perc, 
                                           lost_adapt_perc, lost_rrna_perc, 
                                           lost_host_perc, lost_mask_perc, 
                                           output_perc])) + '\n')
            

@follows(summarizeReadCounts)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
